# LLM_related_paper

## Survey or consluion
- [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842) (Feb 2023) [code]
- [Towards Reasoning in Large Language Models: A Survey.]() 
- [Reasoning with Language Model Prompting: A Survey]()
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) (April 2023)
- [Tool Learning with Foundation Models](https://arxiv.org/abs/2304.08354) (June 2023)

## Foundation Models

[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://scontent-sin6-2.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=qhK-ahCbkBMAX9Dzaw7&_nc_ht=scontent-sin6-2.xx&oh=00_AfCAxDCJgixsboS6n2Wjww-x9hIayo0iASA47IddwbgnJw&oe=64BE66FF)

[GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)

[GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)

## Chain-of-thoughts

- [[self-COT]Self-consistency improves chain of thought reasoning in language models](https://arxiv.org/abs/2203.11171) [code]()
- [Reflexion: an autonomous agent with dynamic memory and self-reflection]()
- [Universality and Limitations of Prompt Tuning](https://arxiv.org/abs/2305.18787) (May 2023)
- [Self-Critique Prompting with Large Language Models for Inductive Instructions](https://arxiv.org/abs/2305.13733) (May 2023, 自我批评)
- [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://arxiv.org/abs/2305.04091v3) (May 2023)
- [Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs](https://arxiv.org/abs/2305.11860) (May 2023)
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) (May 2023)
- [Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models](https://arxiv.org/abs/2305.10276) (May 2023)  
- [Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling](https://arxiv.org/abs/2305.09993) (May 2023) [采样算法更新]
- [Boosted Prompt Ensembles for Large Language Models](https://arxiv.org/abs/2304.05970) (April 2023)
- [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/abs/2304.01904) (April 2023)
- [Reflexion: an autonomous agent with dynamic memory and self-reflection](https://arxiv.org/abs/2303.11366) (March 2023)
- [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651v1) (Mar 2023)
- [Fairness-guided Few-shot Prompting for Large Language Models](https://arxiv.org/abs/2303.13217) (Mar 2023)
- [Tab-CoT: Zero-shot Tabular Chain of Thought](https://arxiv.org/abs/2305.17812) (May 2023)
- [Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance](https://arxiv.org/abs/2305.17306) (May 2023)
- [SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/abs/2305.17390v1) (May 2023) [快慢思维]
- [Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings](https://arxiv.org/abs/2305.02317v1) (May 2023)
- [Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction](https://arxiv.org/abs/2305.02466v1) (May 2023)
- [The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning](https://arxiv.org/abs/2305.14045) (May 2023)
- [Active Learning Principles for In-Context Learning with Large Language Models](https://arxiv.org/abs/2305.14264) (May 2023)
- [Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs](https://arxiv.org/abs/2305.14279) (May 2023)
- [Improving Factuality and Reasoning in Language Models through Multiagent Debate](https://arxiv.org/abs/2305.14325) (May 2023)
- [ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on\\ Chat-based Large Language Models](https://arxiv.org/abs/2305.14323) (May 2023)

## Finetune
### base
- [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention]()
- [LoRA: Low-Rank Adaptation of Large Language Models]()
- [SQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL]()
- [Focused Prefix Tuning for Controllable Text Generation](https://arxiv.org/abs/2306.00369) (June 2023)

### multimodal

## some principle

- [Meta-in-context learning in large language models](https://arxiv.org/abs/2305.12907) (May 2023)
- [Post Hoc Explanations of Language Models Can Improve Language Models](https://arxiv.org/abs/2305.11426) (May 2023)  
- [What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning](https://arxiv.org/abs/2305.09731) (May 2023)
- [Heterogeneous Value Evaluation for Large Language Models](https://arxiv.org/abs/2305.17147) (May 2023)
- [Large Language Models as Tool Makers](https://arxiv.org/abs/2305.17126v1) (May 2023)
- [NPPrompt: Pre-trained Language Models Can be Fully Zero-Shot Learners](https://arxiv.org/abs/2212.06950) (Zero-Shot)

## Agent
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761) [code]

- [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671) [code]

- [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580) [code]

- [Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow](https://arxiv.org/abs/2306.07209) (June 2023，数据分析agent)

- [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291) （May 2023，游戏agent）

- [CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge](https://arxiv.org/abs/2305.09955) (May 2023)

- [Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents](https://arxiv.org/abs/2306.03314)

- [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance](https://arxiv.org/abs/2305.05176v1) (May 2023)[成本与性能]

- AgentGPT [code](https://github.com/reworkd/AgentGPT)

- AutoGPT [[code](https://github.com/Significant-Gravitas/Auto-GPT)]

  

## Application


### code
#### SQL
- [SQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL](https://arxiv.org/abs/2306.00739) (June 2023)
- [Exploring Chain-of-Thought Style Prompting for Text-to-SQL](https://arxiv.org/abs/2305.14215) (May 2023)


### writing
- ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing


### document
- [PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents](https://arxiv.org/abs/2305.14564v1) (May 2023)

### reason
- [Interpretable Math Word Problem Solution Generation Via Step-by-step Planning](https://arxiv.org/abs/2306.00784) (June 2023)
- [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050) (May 2023)
- [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992v1) (May 2023)
- [Why think step-by-step? Reasoning emerges from the locality of experience](https://arxiv.org/abs/2304.03843) (April 2023)


### struct-data
- StructGPT: A General Framework for Large Language Model to Reason over Structured Data
- DiffuD2T: Empowering Data-to-Text Generation with Diffusion(看起来是表格数据)
- [PRODIGY: Enabling In-context Learning Over Graphs](https://arxiv.org/abs/2305.12600v1) (May 2023)

### classification
- [TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks](https://arxiv.org/abs/2305.11430) (May 2023)  
- [PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training](https://arxiv.org/abs/2305.13723) (May 2023)
- [Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation](https://arxiv.org/abs/2305.13785) (May 2023)

  